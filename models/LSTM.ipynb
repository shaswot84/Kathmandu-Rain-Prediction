{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"boxR82M3IVWb"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from torch.utils.data import DataLoader, Dataset\n","from itertools import product"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zF257YO0IcQO"},"outputs":[],"source":["df = pd.read_csv(\"/content/drive/MyDrive/Kathmandu-Precipitation/data/Outlier-removed-dataset.csv\")\n","features = [ i for i in df.columns if i not in [\"precipitation\",\"datetime\"]]\n","target = [\"precipitation\"]\n","X = df[features]\n","y = df[target]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1752332928410,"user":{"displayName":"Shaswot Poudyal","userId":"10950154078121976776"},"user_tz":-345},"id":"fs8THo7cJJA2","outputId":"ba3a148e-5640-4566-f0f2-98feee15a5f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1707, 7)\n","(1707, 1)\n"]}],"source":["scaler_features = StandardScaler()\n","scaler_target = StandardScaler()\n","features_scaled = scaler_features.fit_transform(X)\n","target_scaled = scaler_target.fit_transform(y).flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z2_jjrMJJEhR"},"outputs":[],"source":["# Create sequences for LSTM\n","def create_sequences(data, target, seq_length):\n","    xs, ys = [], []\n","    for i in range(len(data) - seq_length):\n","        x = data[i:i + seq_length]\n","        y = target[i + seq_length]\n","        xs.append(x)\n","        ys.append(y)\n","    return np.array(xs), np.array(ys)\n","\n","seq_length = 10  # Number of time steps in each sequence\n","X, y = create_sequences(features_scaled, target_scaled, seq_length)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_X3hDVVKUg_"},"outputs":[],"source":["# Split into training, validation, and testing sets (rolling window)\n","train_size = int(len(X) * 0.7)\n","val_size = int(len(X) * 0.15)\n","test_size = len(X) - train_size - val_size\n","\n","X_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]\n","y_train, y_val, y_test = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]\n","\n","# Convert to PyTorch tensors\n","X_train = torch.tensor(X_train, dtype=torch.float32)\n","y_train = torch.tensor(y_train, dtype=torch.float32)\n","X_val = torch.tensor(X_val, dtype=torch.float32)\n","y_val = torch.tensor(y_val, dtype=torch.float32)\n","X_test = torch.tensor(X_test, dtype=torch.float32)\n","y_test = torch.tensor(y_test, dtype=torch.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7W__1nOPKWl2"},"outputs":[],"source":["class LSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers, output_size):\n","        super(LSTMModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        # LSTM layer\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","\n","        # Fully connected layer\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        # Initialize hidden state and cell state\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","\n","        # Forward propagate LSTM\n","        out, _ = self.lstm(x, (h0, c0))\n","\n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aL_6R0yXMhRK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51239,"status":"ok","timestamp":1752333023017,"user":{"displayName":"Shaswot Poudyal","userId":"10950154078121976776"},"user_tz":-345},"id":"_sPuWth2LJes","outputId":"64fca1ec-b301-4b55-b307-76fb372a2026"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/20], Training Loss: 0.8834\n","Epoch [2/20], Training Loss: 0.8290\n","Epoch [3/20], Training Loss: 0.8275\n","Epoch [4/20], Training Loss: 0.8249\n","Epoch [5/20], Training Loss: 0.8235\n","Epoch [6/20], Training Loss: 0.8224\n","Epoch [7/20], Training Loss: 0.8213\n","Epoch [8/20], Training Loss: 0.8203\n","Epoch [9/20], Training Loss: 0.8194\n","Epoch [10/20], Training Loss: 0.8184\n","Epoch [11/20], Training Loss: 0.8175\n","Epoch [12/20], Training Loss: 0.8164\n","Epoch [13/20], Training Loss: 0.8153\n","Epoch [14/20], Training Loss: 0.8140\n","Epoch [15/20], Training Loss: 0.8127\n","Epoch [16/20], Training Loss: 0.8112\n","Epoch [17/20], Training Loss: 0.8097\n","Epoch [18/20], Training Loss: 0.8082\n","Epoch [19/20], Training Loss: 0.8064\n","Epoch [20/20], Training Loss: 0.8042\n","Params: (32, 1, 0.001), Validation MSE: 169.6379, Validation RMSE: 13.0245\n","Epoch [1/20], Training Loss: 0.8489\n","Epoch [2/20], Training Loss: 0.8401\n","Epoch [3/20], Training Loss: 0.8379\n","Epoch [4/20], Training Loss: 0.8336\n","Epoch [5/20], Training Loss: 0.8277\n","Epoch [6/20], Training Loss: 0.8232\n","Epoch [7/20], Training Loss: 0.8479\n","Epoch [8/20], Training Loss: 0.8186\n","Epoch [9/20], Training Loss: 0.8294\n","Epoch [10/20], Training Loss: 0.8185\n","Epoch [11/20], Training Loss: 0.8131\n","Epoch [12/20], Training Loss: 0.8087\n","Epoch [13/20], Training Loss: 0.8021\n","Epoch [14/20], Training Loss: 0.7941\n","Epoch [15/20], Training Loss: 0.7863\n","Epoch [16/20], Training Loss: 0.7736\n","Epoch [17/20], Training Loss: 0.7677\n","Epoch [18/20], Training Loss: 0.7718\n","Epoch [19/20], Training Loss: 0.7489\n","Epoch [20/20], Training Loss: 0.7416\n","Params: (32, 1, 0.01), Validation MSE: 168.7246, Validation RMSE: 12.9894\n","Epoch [1/20], Training Loss: 0.8503\n","Epoch [2/20], Training Loss: 0.8390\n","Epoch [3/20], Training Loss: 0.8298\n","Epoch [4/20], Training Loss: 0.8285\n","Epoch [5/20], Training Loss: 0.8273\n","Epoch [6/20], Training Loss: 0.8264\n","Epoch [7/20], Training Loss: 0.8256\n","Epoch [8/20], Training Loss: 0.8249\n","Epoch [9/20], Training Loss: 0.8242\n","Epoch [10/20], Training Loss: 0.8235\n","Epoch [11/20], Training Loss: 0.8227\n","Epoch [12/20], Training Loss: 0.8219\n","Epoch [13/20], Training Loss: 0.8208\n","Epoch [14/20], Training Loss: 0.8196\n","Epoch [15/20], Training Loss: 0.8181\n","Epoch [16/20], Training Loss: 0.8166\n","Epoch [17/20], Training Loss: 0.8151\n","Epoch [18/20], Training Loss: 0.8138\n","Epoch [19/20], Training Loss: 0.8126\n","Epoch [20/20], Training Loss: 0.8115\n","Params: (32, 2, 0.001), Validation MSE: 172.8155, Validation RMSE: 13.1459\n","Epoch [1/20], Training Loss: 0.8559\n","Epoch [2/20], Training Loss: 0.8291\n","Epoch [3/20], Training Loss: 0.8325\n","Epoch [4/20], Training Loss: 0.8303\n","Epoch [5/20], Training Loss: 0.8276\n","Epoch [6/20], Training Loss: 0.8262\n","Epoch [7/20], Training Loss: 0.8237\n","Epoch [8/20], Training Loss: 0.8203\n","Epoch [9/20], Training Loss: 0.8213\n","Epoch [10/20], Training Loss: 0.8183\n","Epoch [11/20], Training Loss: 0.8163\n","Epoch [12/20], Training Loss: 0.8094\n","Epoch [13/20], Training Loss: 0.8201\n","Epoch [14/20], Training Loss: 0.8112\n","Epoch [15/20], Training Loss: 0.7985\n","Epoch [16/20], Training Loss: 0.7843\n","Epoch [17/20], Training Loss: 0.7655\n","Epoch [18/20], Training Loss: 0.7642\n","Epoch [19/20], Training Loss: 0.7553\n","Epoch [20/20], Training Loss: 0.7398\n","Params: (32, 2, 0.01), Validation MSE: 163.7505, Validation RMSE: 12.7965\n","Epoch [1/20], Training Loss: 0.8465\n","Epoch [2/20], Training Loss: 0.8322\n","Epoch [3/20], Training Loss: 0.8259\n","Epoch [4/20], Training Loss: 0.8244\n","Epoch [5/20], Training Loss: 0.8231\n","Epoch [6/20], Training Loss: 0.8219\n","Epoch [7/20], Training Loss: 0.8207\n","Epoch [8/20], Training Loss: 0.8196\n","Epoch [9/20], Training Loss: 0.8184\n","Epoch [10/20], Training Loss: 0.8170\n","Epoch [11/20], Training Loss: 0.8156\n","Epoch [12/20], Training Loss: 0.8138\n","Epoch [13/20], Training Loss: 0.8117\n","Epoch [14/20], Training Loss: 0.8090\n","Epoch [15/20], Training Loss: 0.8056\n","Epoch [16/20], Training Loss: 0.8023\n","Epoch [17/20], Training Loss: 0.7999\n","Epoch [18/20], Training Loss: 0.7976\n","Epoch [19/20], Training Loss: 0.7952\n","Epoch [20/20], Training Loss: 0.7919\n","Params: (64, 1, 0.001), Validation MSE: 166.9797, Validation RMSE: 12.9221\n","Epoch [1/20], Training Loss: 0.8940\n","Epoch [2/20], Training Loss: 0.8331\n","Epoch [3/20], Training Loss: 0.8329\n","Epoch [4/20], Training Loss: 0.8320\n","Epoch [5/20], Training Loss: 0.8285\n","Epoch [6/20], Training Loss: 0.8277\n","Epoch [7/20], Training Loss: 0.8228\n","Epoch [8/20], Training Loss: 0.8185\n","Epoch [9/20], Training Loss: 0.8160\n","Epoch [10/20], Training Loss: 0.8070\n","Epoch [11/20], Training Loss: 0.8106\n","Epoch [12/20], Training Loss: 0.8056\n","Epoch [13/20], Training Loss: 0.8010\n","Epoch [14/20], Training Loss: 0.7801\n","Epoch [15/20], Training Loss: 0.7796\n","Epoch [16/20], Training Loss: 0.7525\n","Epoch [17/20], Training Loss: 0.7644\n","Epoch [18/20], Training Loss: 0.7351\n","Epoch [19/20], Training Loss: 0.7009\n","Epoch [20/20], Training Loss: 0.6772\n","Params: (64, 1, 0.01), Validation MSE: 169.8635, Validation RMSE: 13.0332\n","Epoch [1/20], Training Loss: 0.8445\n","Epoch [2/20], Training Loss: 0.8348\n","Epoch [3/20], Training Loss: 0.8326\n","Epoch [4/20], Training Loss: 0.8290\n","Epoch [5/20], Training Loss: 0.8272\n","Epoch [6/20], Training Loss: 0.8258\n","Epoch [7/20], Training Loss: 0.8245\n","Epoch [8/20], Training Loss: 0.8231\n","Epoch [9/20], Training Loss: 0.8217\n","Epoch [10/20], Training Loss: 0.8200\n","Epoch [11/20], Training Loss: 0.8181\n","Epoch [12/20], Training Loss: 0.8159\n","Epoch [13/20], Training Loss: 0.8134\n","Epoch [14/20], Training Loss: 0.8103\n","Epoch [15/20], Training Loss: 0.8065\n","Epoch [16/20], Training Loss: 0.8027\n","Epoch [17/20], Training Loss: 0.7995\n","Epoch [18/20], Training Loss: 0.7956\n","Epoch [19/20], Training Loss: 0.7924\n","Epoch [20/20], Training Loss: 0.7966\n","Params: (64, 2, 0.001), Validation MSE: 173.4851, Validation RMSE: 13.1714\n","Epoch [1/20], Training Loss: 0.8827\n","Epoch [2/20], Training Loss: 0.8335\n","Epoch [3/20], Training Loss: 0.8344\n","Epoch [4/20], Training Loss: 0.8344\n","Epoch [5/20], Training Loss: 0.8326\n","Epoch [6/20], Training Loss: 0.8274\n","Epoch [7/20], Training Loss: 0.8239\n","Epoch [8/20], Training Loss: 0.8222\n","Epoch [9/20], Training Loss: 0.8223\n","Epoch [10/20], Training Loss: 0.8183\n","Epoch [11/20], Training Loss: 0.8130\n","Epoch [12/20], Training Loss: 0.8232\n","Epoch [13/20], Training Loss: 0.8203\n","Epoch [14/20], Training Loss: 0.8003\n","Epoch [15/20], Training Loss: 0.7792\n","Epoch [16/20], Training Loss: 0.7891\n","Epoch [17/20], Training Loss: 0.7665\n","Epoch [18/20], Training Loss: 0.7695\n","Epoch [19/20], Training Loss: 0.7536\n","Epoch [20/20], Training Loss: 0.7780\n","Params: (64, 2, 0.01), Validation MSE: 156.7769, Validation RMSE: 12.5211\n","Best Params: (64, 2, 0.01), Best Validation RMSE: 12.5211\n"]}],"source":["# Hyperparameter grid\n","param_grid = {\n","    'hidden_size': [32, 64],\n","    'num_layers': [1, 2],\n","    'learning_rate': [0.001, 0.01]\n","}\n","\n","best_rmse = float('inf')\n","best_params = None\n","\n","for params in product(*param_grid.values()):\n","    hidden_size, num_layers, learning_rate = params\n","\n","    # Instantiate the model\n","    model = LSTMModel(input_size=7, hidden_size=hidden_size, num_layers=num_layers, output_size=1)\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Training loop\n","    num_epochs = 20\n","    batch_size = 32\n","    train_data = torch.utils.data.TensorDataset(X_train, y_train)\n","    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        for inputs, targets in train_loader:\n","            outputs = model(inputs)\n","            loss = criterion(outputs.squeeze(), targets)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        # Print training loss\n","        avg_train_loss = running_loss / len(train_loader)\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}\")\n","\n","    # Evaluate on validation set\n","    model.eval()\n","    with torch.no_grad():\n","        val_predictions = model(X_val).squeeze().numpy()\n","        val_predictions_rescaled = scaler_target.inverse_transform(val_predictions.reshape(-1, 1)).flatten()\n","        y_val_rescaled = scaler_target.inverse_transform(y_val.numpy().reshape(-1, 1)).flatten()\n","\n","        val_mse = mean_squared_error(y_val_rescaled, val_predictions_rescaled)\n","        val_rmse = np.sqrt(val_mse)\n","\n","    print(f\"Params: {params}, Validation MSE: {val_mse:.4f}, Validation RMSE: {val_rmse:.4f}\")\n","\n","    # Save best parameters\n","    if val_rmse \u003c best_rmse:\n","        best_rmse = val_rmse\n","        best_params = params\n","\n","print(f\"Best Params: {best_params}, Best Validation RMSE: {best_rmse:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13635,"status":"ok","timestamp":1752333039726,"user":{"displayName":"Shaswot Poudyal","userId":"10950154078121976776"},"user_tz":-345},"id":"pAQI-yF5KcdA","outputId":"dce69b1a-2302-472f-e6fb-054c236aeeaf"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/20], Training Loss: 0.8564\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2/20], Training Loss: 0.8117\n","Epoch [3/20], Training Loss: 0.8093\n","Epoch [4/20], Training Loss: 0.8066\n","Epoch [5/20], Training Loss: 0.8009\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [6/20], Training Loss: 0.7944\n","Epoch [7/20], Training Loss: 0.7931\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [8/20], Training Loss: 0.7862\n","Epoch [9/20], Training Loss: 0.7818\n","Epoch [10/20], Training Loss: 0.7844\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [11/20], Training Loss: 0.7757\n","Epoch [12/20], Training Loss: 0.7666\n","Epoch [13/20], Training Loss: 0.7577\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [14/20], Training Loss: 0.7448\n","Epoch [15/20], Training Loss: 0.7492\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [16/20], Training Loss: 0.7697\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [17/20], Training Loss: 0.7413\n","Epoch [18/20], Training Loss: 0.7039\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [19/20], Training Loss: 0.7132\n","Epoch [20/20], Training Loss: 0.7108\n","Test Metrics:\n","  Test MSE: 208.9143\n","  Test RMSE: 14.4539\n","  Test MAE: 7.4160\n"]}],"source":["# Retrain with best parameters\n","hidden_size, num_layers, learning_rate = best_params\n","model = LSTMModel(input_size=7, hidden_size=hidden_size, num_layers=num_layers, output_size=1)\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Combine training and validation sets\n","X_train_full = torch.cat((X_train, X_val), dim=0)\n","y_train_full = torch.cat((y_train, y_val), dim=0)\n","train_data = torch.utils.data.TensorDataset(X_train_full, y_train_full)\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n","\n","# Train the model\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, targets in train_loader:\n","        outputs = model(inputs)\n","        loss = criterion(outputs.squeeze(), targets)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss/len(train_loader):.4f}\")\n","\n","# Evaluate on test set\n","model.eval()\n","with torch.no_grad():\n","    test_predictions = model(X_test).squeeze().numpy()\n","    test_predictions_rescaled = scaler_target.inverse_transform(test_predictions.reshape(-1, 1)).flatten()\n","    y_test_rescaled = scaler_target.inverse_transform(y_test.numpy().reshape(-1, 1)).flatten()\n","\n","    test_mse = mean_squared_error(y_test_rescaled, test_predictions_rescaled)\n","    test_rmse = np.sqrt(test_mse)\n","    test_mae = mean_absolute_error(y_test_rescaled, test_predictions_rescaled)\n","\n","print(f\"Test Metrics:\")\n","print(f\"  Test MSE: {test_mse:.4f}\")\n","print(f\"  Test RMSE: {test_rmse:.4f}\")\n","print(f\"  Test MAE: {test_mae:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46,"status":"ok","timestamp":1752332695881,"user":{"displayName":"Shaswot Poudyal","userId":"10950154078121976776"},"user_tz":-345},"id":"jDniZOQrKdtu","outputId":"2d287efa-0373-479e-929c-b62d36d55751"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test RMSE: 15.0719\n"]}],"source":["model.eval()\n","with torch.no_grad():\n","    predictions = model(X_test).squeeze().numpy()\n","    predictions_rescaled = scaler_target.inverse_transform(predictions.reshape(-1, 1)).flatten()\n","    y_test_rescaled = scaler_target.inverse_transform(y_test.numpy().reshape(-1, 1)).flatten()\n","\n","# Calculate evaluation metrics (e.g., RMSE)\n","from sklearn.metrics import mean_squared_error\n","rmse = np.sqrt(mean_squared_error(y_test_rescaled, predictions_rescaled))\n","print(f\"Test RMSE: {rmse:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kh2y-BZcKveX"},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyOBm/Ko5ZY6F8jwkW2s1rT8","gpuType":"V28","mount_file_id":"16lHW4t6koYz6IMx57GFyVjLSPnAEhudc","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}